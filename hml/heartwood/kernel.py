# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks_dev/kernel.ipynb (unless otherwise specified).

__all__ = ['sigmoid', 'make_bimodal_assymetric_regression', 'sparsify', 'hstack', 'vstack', 'stack', 'RobustEncoder',
           'PrefitEstimator', 'BaseMixedForest', 'HeterogeneousMixedForest', 'MixedForestRegressor',
           'MixedForestClassifier', 'ForestBipartiteGraphTransformer', 'EstimatorKernel', 'JaccardForestKernel',
           'CategoricalLinearKernel', 'ClassificationLinearBottleneck', 'RegressionLinearBottleneck', 'MLPKernel',
           'BOWKernel', 'DiscretizedTargetKernel']

# Cell
from functools import reduce

from typing import Union

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import normalize, OneHotEncoder, OrdinalEncoder, KBinsDiscretizer
from sklearn.pipeline import make_pipeline
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.decomposition import TruncatedSVD

from scipy import sparse
import numpy as np

from nmslearn.neighbors import FastJaccardNN, FastKLDivNN, FastL2NN
from .utils import hstack, RobustEncoder

# Cell
from sklearn.datasets import make_regression

def sigmoid(x):
    return 1/(1+np.exp(x))

def make_bimodal_assymetric_regression(
    n_samples=100000,
    bimodal_factor_weight = 2,
    n_features=15,
    n_informative=6,
    n_targets=2,
    bias=500,
    effective_rank=None,
    tail_strength=10,
    noise=150,
    shuffle=True,
    coef=False,
    random_state=None
):

    X,y = make_regression(
        n_samples=n_samples,
        n_features=n_features,
        n_informative=n_informative,
        n_targets=n_targets,
        bias=bias,
        effective_rank=effective_rank,
        tail_strength=tail_strength,
        noise=noise,
        shuffle=shuffle,
        coef=coef,
        random_state=random_state
    )


    #make one of X[1] feature mode weightening
    bimodal_factors = (sigmoid(bimodal_factor_weight*X[:,-1]) > np.random.random(size = X.shape[0])).astype(int)
    bimodal_factors[bimodal_factors == 0] = -1
    bimodal_factors = bimodal_factors.reshape(-1,1)

    y = bimodal_factors*y

    return X,y

# Cell
def sparsify(*arrs):
    '''
    makes input arrs sparse
    '''
    arrs = list(arrs)
    for i in range(len(arrs)):
        if not sparse.issparse(arrs[i]):
            arrs[i] = sparse.csr_matrix(arrs[i])

    return arrs

def _robust_stack(blocks, stack_method = 'stack', **kwargs):

    if any(sparse.issparse(i) for i in blocks):
        stacked = getattr(sparse, stack_method)(blocks, **kwargs)
    else:
        stacked = getattr(np, stack_method)(blocks, **kwargs)
    return stacked

def hstack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'hstack', **kwargs)

def vstack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'vstack', **kwargs)

def stack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'stack', **kwargs)


class RobustEncoder(BaseEstimator, TransformerMixin):

    def __init__(self,):
        '''
        A robust one hot encoder. Always return the same amount of nonzero value sin each transformed row.
        Has columns for unknown values
        '''
        return

    def fit(self, X, y = None, **kwawrgs):
        self.ordinalencoder_ = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1).fit(X)

        X = self.ordinalencoder_.transform(X)

        categories = [np.arange(-1, len(cats)) for cats in self.ordinalencoder_.categories_]
        self.onehotencoder_ = OneHotEncoder(categories = categories).fit(X)
        return self

    def transform(self, X, **kwargs):
        X = self.ordinalencoder_.transform(X)
        return self.onehotencoder_.transform(X)

# Cell

class PrefitEstimator(BaseEstimator):

    def __init__(self, prefit_estimator):
        self.prefit_estimator = prefit_estimator
        self.is_fitted_ = True
        return

    def __getattr__(self, attr):
        '''
        gets the attributes from prefit_estimator, except if the attribute (or method)
        is "fit".

        if the "transform" or "predict" method is called, it'll return self.prefit_estimator's method
        '''
        if attr == 'fit':
            return self.fit
        elif attr == 'fit_transform':
            return self.fit_transform
        elif attr == 'fit_predict':
            return self.fit_predict
        else:
            return getattr(self.prefit_estimator, attr)

    def fit(self, X, y = None, **kwargs):
        '''
        the fit method does nothing (since prefit_estimator is already fitted) and returns self.
        '''
        return self

    def fit_transform(self, X, y = None, **kwargs):
        return self.transform(X) #will get "transform" method from self.prefit_estimator

    def fit_predict(self, X, y = None, **kwargs):
        return self.predict(X) #will get "predict" method from self.prefit_estimator

# Cell
from sklearn.utils.validation import check_is_fitted

class BaseMixedForest(BaseEstimator):

    def __init__(
        self,
        estimators,
        estimators_weights = None,
        fully_supervised = True,
        use_already_fitted = False,
        biadjecency_weights = 'uniform',
        stack_outputs = True,
    ):
        '''
        a heterogeneous ensemble of forests
        '''
        self.estimators = estimators
        self.estimators_weights = estimators_weights
        self.fully_supervised = fully_supervised
        self.use_already_fitted = use_already_fitted
        self.biadjecency_weights = biadjecency_weights
        self.stack_outputs = stack_outputs
        return

    def __getattr__(self, attr):
        return [getattr(i, attr) for i in self.estimators]

    def _iter_apply(self, method, *args, **kwargs):

        result = []
        for estim in self.estimators:
            result.append(getattr(estim, method)(*args, **kwargs))
        return result

    def apply(self, X, stack = None, **kwargs):

        result = self._iter_apply(method = 'apply', X = X)

        if stack is None:
            stack = self.stack_outputs
        #handle boosting case where returned array has 3 dims
        for arr in result:
            if len(arr.shape) >= 3:
                arr = arr.reshape(arr.shape[0], arr.shape[1]*arr.shape[2])

        if stack:
            result = hstack(result)
        return result

    def decision_path(self, X, stack = None, **kwargs):
        result = self._iter_apply(method = 'decision_path', X = X)

        if stack is None:
            stack = self.stack_outputs

        if stack:
            result0 = hstack([i[0] for i in result]).tocsr()
            result1 = hstack([i[1] for i in result])
        else:
            result0,result1 = result

        return result0, result1

    def node_biadjecency_matrix(self, X, use_weights = None, stack = None):

        '''
        use_weights can be "n_estimators", "weighted" or "uniform"
        unweighted returns biadjecency matrix filled with ones or zeros
        uniform makes the total sum of edge weights from all estimators the same
        wieghted
        '''

        if use_weights is None:
            use_weights = self.biadjecency_weights
        if stack is None:
            stack = self.stack_outputs

        #invert natural weights to make the sum of edges in each biadjacency uniform accross all estimators
        natural_weights = np.array([i.n_estimators for i in self.estimators])
        natural_weights = natural_weights/natural_weights.sum()

        uniform_weights = (1/natural_weights)
        uniform_weights = uniform_weights/uniform_weights.sum()

        if use_weights == "uniform":
            weights = uniform_weights
        elif use_weights == "weighted":
            weights = uniform_weights*self.estimators_weights_
        elif use_weights == "n_estimators":
            weights = np.ones((len(self.estimators_weights_),))
        else:
            raise ValueError(f'use_weights should be one of ["uniform", "weighted", "n_estimators", None], got {use_weights}')

        terminal_nodes = self.apply(X, stack=False)
        biadjs = []
        for i in range(len(terminal_nodes)):
            biadj_i = self.one_hot_node_embeddings_encoders_[i].transform(terminal_nodes[i])
            #scale and append
            biadjs.append(biadj_i*weights[i])

        if stack:
            biadjs = hstack(biadjs)

        return biadjs

# Cell

#export
class HeterogeneousMixedForest(BaseMixedForest):

    def fit(self, X, y = None, sample_weight = None):

        #get "natural" weights of estimators
        natural_weights = np.array([i.n_estimators for i in self.estimators])
        natural_weights = natural_weights/natural_weights.sum()
        #set estimator weights
        if self.estimators_weights is None:
            weights = natural_weights
        else:
            if len(self.estimators_weights) != len(self.estimators):
                raise ValueError(f'Shape mismatch between estimators and weights ({len(self.estimators)} != {len(self.estimators_weights)})')
            weights = np.array(weights)
            weights = weights/weights.sum()


        self.natural_weights_ = natural_weights
        self.estimators_weights_ = weights
        self.classes_ = classes
        self.multilabel_ = multilabel
        self.output_dim_ = output_dim

        #fit one hot encoders of the nodes
        terminal_nodes = super().apply(X, stack = False)
        self.one_hot_node_embeddings_encoders_ = [OneHotEncoder().fit(xi) for xi in terminal_nodes]
        return self

# Cell
class MixedForestRegressor(BaseMixedForest):

    def fit(self, X, y = None, sample_weight = None):

        #check if multidim output
        if len(y.shape) > 1:
            output_dim = y.shape[-1]
        else:
            output_dim = 1

        #get "natural" weights of estimators
        natural_weights = np.array([i.n_estimators for i in self.estimators])
        natural_weights = natural_weights/natural_weights.sum()
        #set estimator weights
        if self.estimators_weights is None:
            weights = natural_weights
        else:
            if len(self.estimators_weights) != len(self.estimators):
                raise ValueError(f'Shape mismatch between estimators and weights ({len(self.estimators)} != {len(self.estimators_weights)})')
            weights = np.array(weights)
            weights = weights/weights.sum()

        #fit estimators if needed
        for estim in self.estimators:
            if self.use_already_fitted:
                if not check_is_fitted(estim):
                    estim.fit(X, y = y, sample_weight = sample_weight)
                else:
                    warn(f"{estim} is already fitted and use_already_fitted was set to True in the constructor. The estimator won't be fitted, so ensure compatibility of inputs and outputs.")
            else:
                estim.fit(X, y = y, sample_weight = sample_weight)

        self.natural_weights_ = natural_weights
        self.estimators_weights_ = weights
        self.output_dim_ = output_dim

        #fit one hot encoders of the nodes
        terminal_nodes = super().apply(X, stack = False)
        self.one_hot_node_embeddings_encoders_ = [OneHotEncoder().fit(xi) for xi in terminal_nodes]
        return self


    def predict(self, X, aggregate = True):
        '''
        predicts classes
        '''

        weights = self.estimators_weights_
        result = super()._iter_apply(method = 'predict', X = X)
        sum_f = lambda a,b : weights[a]*result[a] + weights[b]*result[b]

        if aggregate:
            result = reduce(sum_f, range(len(result)))
        else:
            result = hstack([i.reshape(-1,1) for i in result])

        return result

# Cell
class MixedForestClassifier(BaseMixedForest):

    def fit(self, X, y = None, sample_weight = None):

        #check if multidim output
        if len(y.shape) > 1:
            output_dim = y.shape[-1]
        else:
            output_dim = 1

        if output_dim > 1:
            multilabel = True
        else:
            multilabel = False

        #get "natural" weights of estimators
        natural_weights = np.array([i.n_estimators for i in self.estimators])
        natural_weights = natural_weights/natural_weights.sum()
        #set estimator weights
        if self.estimators_weights is None:
            weights = natural_weights
        else:
            if len(self.estimators_weights) != len(self.estimators):
                raise ValueError(f'Shape mismatch between estimators and weights ({len(self.estimators)} != {len(self.estimators_weights)})')
            weights = np.array(weights)
            weights = weights/weights.sum()

        #fit estimators if needed
        for estim in self.estimators:
            if self.use_already_fitted:
                if not check_is_fitted(estim):
                    estim.fit(X, y = y, sample_weight = sample_weight)
                else:
                    warn(f"{estim} is already fitted and use_already_fitted was set to True in the constructor. The estimator won't be fitted, so ensure compatibility of inputs and outputs.")
            else:
                estim.fit(X, y = y, sample_weight = sample_weight)

        #check if classes are the same accross estimators
        if self.fully_supervised:
            classes = [estim.classes_ for estim in self.estimators]

            if multilabel:
                for i in range(len(classes)-1):
                    for j in range(len(classes[i])):
                        assert np.all(classes[i][j] == classes[i+1][j]), f'different classes in estimators: {classes[i]} and {classes[i+1]}'
                classes = classes[0]

            else:
                for i in range(len(classes)-1):
                    assert np.all(classes[i] == classes[i+1]), f'different classes in estimators: {classes[i]} and {classes[i+1]}'
                classes = classes[0]
        else:
            classes = None

        self.natural_weights_ = natural_weights
        self.estimators_weights_ = weights
        self.classes_ = classes
        self.multilabel_ = multilabel
        self.output_dim_ = output_dim

        #fit one hot encoders of the nodes
        terminal_nodes = super().apply(X, stack = False)
        self.one_hot_node_embeddings_encoders_ = [OneHotEncoder().fit(xi) for xi in terminal_nodes]
        return self

    def predict(self, X):
        '''
        predicts classes
        '''
        if self.multilabel_:
            probas = self.predict_proba(X)
            labels = []
            for i in range(len(probas)):
                indices = np.argmax(probas[i], axis = 1)
                labels.append(self.classes_[i][indices])
            labels = np.array(labels).T
        else:
            probas = self.predict_proba(X)
            indices = np.argmax(probas, axis = 1)
            labels = self.classes_[indices]

        return labels


    def predict_proba(self, X):
        '''
        predicts proba of each class
        '''
        weights = self.estimators_weights_
        result = self._iter_apply(method = 'predict_proba', X = X)
        sum_f = lambda a,b : weights[a]*result[a] + weights[b]*result[b]
        if self.multilabel_:
            #reverse the order of arrays in list
            res = [[] for _ in range(self.output_dim_)]
            for i in range(len(result)):
                for j in range(self.output_dim_):
                    res[j].append(result[i][j])

            result = []
            for dim_result in res:
                sum_f = lambda a,b : weights[a]*dim_result[a] + weights[b]*dim_result[b]
                res_i = reduce(sum_f, range(len(dim_result)))
                result.append(res_i)

        else:
            result = reduce(sum_f, range(len(result)))

        return result

# Cell
from sknetwork.embedding import GSVD, Spectral, SVD, PCA, RandomProjection, LouvainEmbedding, LouvainNE, ForceAtlas
from .utils import sparse_dot_product
class ForestBipartiteGraphTransformer(BaseEstimator):
    #fit models and graph embeddings on terminal node space
    #save louvain terminal node embeddings for inference
    #transform method will yield louvain embeddings of the point
    #create get_label method

    def __init__(
        self,
        forest_estimator,
        embedding_method = 'louvain',
        return_sparse = False,
        **embedding_kws
    ):
        self.forest_estimator = forest_estimator
        self.embedding_method = embedding_method
        self.return_sparse = return_sparse
        self.embedding_kws = embedding_kws
        return

    def __getattr__(self, attr):
        '''
        returns self.forest_estimator attribute if not found in class definition
        '''
        return getattr(self.forest_estimator, attr)

    def fit(self, X, y = None, **kwargs):

        #fit estimator
        self.forest_estimator.fit(X = X, y = y, **kwargs)
        # gets terminal nodes
        terminal_nodes = self.apply(X)
        #fit one hot encoders of the nodes
        self.one_hot_node_embeddings_encoders_ = OneHotEncoder().fit(terminal_nodes)
        #fits emedder
        self.graph_embedder_ = self._fit_embeddings(X, **kwargs)
        return self

    def apply(self, X, **kwargs):
        '''
        applies method "apply" ensuring shape compatibility
        '''
        X = self.forest_estimator.apply(X, **kwargs)
        #handle boosting case
        if len(X.shape) > 2:
            X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])
        return X

    def node_biadjecency_matrix(self, X):

        terminal_nodes = self.apply(X)
        #gets biadjecency matrix
        biadjecency_matrix = self.one_hot_node_embeddings_encoders_.transform(terminal_nodes)
        return biadjecency_matrix

    def _fit_embeddings(self, X, embeddings_method = None, **kwargs):

        '''
        fits embedder and returns fitted object
        '''

        if not kwargs:
            kwargs = self.embedding_kws

        if embeddings_method is None:
            embeddings_method = self.embedding_method

        if embeddings_method == 'louvain':
            method = LouvainEmbedding(**kwargs)
        else:
            raise ValueError(f'Suported methods are: ["louvain"], {embedding_method} was passed.')

        G = self.node_biadjecency_matrix(X)
        graph_embedder = method.fit(G)
        return graph_embedder


    def transform(self, X, return_sparse = None):
        '''
        makes a normalized dot product (linear combination) between terminal nodes of a specific datapoint and the terminal nodes embeddings
        '''
        if return_sparse is None:
            return_sparse = self.return_sparse

        X = self.node_biadjecency_matrix(X)
        terminal_node_embs = sparse.csr_matrix(self.graph_embedder_.embedding_col_)

        #TODO: implement flexible dot product function
        embs = sparse_dot_product(X, terminal_node_embs, terminal_node_embs.shape[-1])/self.n_estimators

        if return_sparse:
            embs = sparse.csr_matrix(embs)
        else:
            if sparse.issparse(embs):
                embs = embs.A
            else:
                pass

        return embs