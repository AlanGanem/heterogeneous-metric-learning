# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks_dev/kernel.ipynb (unless otherwise specified).

__all__ = ['sigmoid', 'make_bimodal_assymetric_regression', 'sparsify', 'hstack', 'vstack', 'stack', 'RobustEncoder',
           'PrefitEstimator', 'BaseMixedForest', 'HeterogeneousMixedForest', 'MixedForestRegressor',
           'MixedForestClassifier', 'ForestBipartiteGraphTransformer', 'EstimatorKernel', 'JaccardForestKernel',
           'CategoricalLinearKernel', 'ClassificationLinearBottleneck', 'RegressionLinearBottleneck', 'MLPKernel',
           'BOWKernel', 'DiscretizedTargetKernel']

# Cell
from functools import reduce

from typing import Union

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import normalize, OneHotEncoder, OrdinalEncoder, KBinsDiscretizer
from sklearn.pipeline import make_pipeline
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.decomposition import TruncatedSVD

from scipy import sparse
import numpy as np

from nmslearn.neighbors import FastJaccardNN, FastKLDivNN, FastL2NN
from .utils import hstack, RobustEncoder

# Cell
from sklearn.datasets import make_regression

def sigmoid(x):
    return 1/(1+np.exp(x))

def make_bimodal_assymetric_regression(
    n_samples=100000,
    bimodal_factor_weight = 2,
    n_features=15,
    n_informative=6,
    n_targets=2,
    bias=500,
    effective_rank=None,
    tail_strength=10,
    noise=150,
    shuffle=True,
    coef=False,
    random_state=None
):

    X,y = make_regression(
        n_samples=n_samples,
        n_features=n_features,
        n_informative=n_informative,
        n_targets=n_targets,
        bias=bias,
        effective_rank=effective_rank,
        tail_strength=tail_strength,
        noise=noise,
        shuffle=shuffle,
        coef=coef,
        random_state=random_state
    )


    #make one of X[1] feature mode weightening
    bimodal_factors = (sigmoid(bimodal_factor_weight*X[:,-1]) > np.random.random(size = X.shape[0])).astype(int)
    bimodal_factors[bimodal_factors == 0] = -1
    bimodal_factors = bimodal_factors.reshape(-1,1)

    y = bimodal_factors*y

    return X,y

# Cell
def sparsify(*arrs):
    '''
    makes input arrs sparse
    '''
    arrs = list(arrs)
    for i in range(len(arrs)):
        if not sparse.issparse(arrs[i]):
            arrs[i] = sparse.csr_matrix(arrs[i])

    return arrs

def _robust_stack(blocks, stack_method = 'stack', **kwargs):

    if any(sparse.issparse(i) for i in blocks):
        stacked = getattr(sparse, stack_method)(blocks, **kwargs)
    else:
        stacked = getattr(np, stack_method)(blocks, **kwargs)
    return stacked

def hstack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'hstack', **kwargs)

def vstack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'vstack', **kwargs)

def stack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'stack', **kwargs)


class RobustEncoder(BaseEstimator, TransformerMixin):

    def __init__(self,):
        '''
        A robust one hot encoder. Always return the same amount of nonzero value sin each transformed row.
        Has columns for unknown values
        '''
        return

    def fit(self, X, y = None, **kwawrgs):
        self.ordinalencoder_ = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1).fit(X)

        X = self.ordinalencoder_.transform(X)

        categories = [np.arange(-1, len(cats)) for cats in self.ordinalencoder_.categories_]
        self.onehotencoder_ = OneHotEncoder(categories = categories).fit(X)
        return self

    def transform(self, X, **kwargs):
        X = self.ordinalencoder_.transform(X)
        return self.onehotencoder_.transform(X)

# Cell

class PrefitEstimator(BaseEstimator):

    def __init__(self, prefit_estimator):
        self.prefit_estimator = prefit_estimator
        self.is_fitted_ = True
        return

    def __getattr__(self, attr):
        '''
        gets the attributes from prefit_estimator, except if the attribute (or method)
        is "fit".

        if the "transform" or "predict" method is called, it'll return self.prefit_estimator's method
        '''
        if attr == 'fit':
            return self.fit
        elif attr == 'fit_transform':
            return self.fit_transform
        elif attr == 'fit_predict':
            return self.fit_predict
        else:
            return getattr(self.prefit_estimator, attr)

    def fit(self, X, y = None, **kwargs):
        '''
        the fit method does nothing (since prefit_estimator is already fitted) and returns self.
        '''
        return self

    def fit_transform(self, X, y = None, **kwargs):
        return self.transform(X) #will get "transform" method from self.prefit_estimator

    def fit_predict(self, X, y = None, **kwargs):
        return self.predict(X) #will get "predict" method from self.prefit_estimator

# Cell
from sklearn.utils.validation import check_is_fitted

class BaseMixedForest(BaseEstimator):

    def __init__(
        self,
        estimators,
        estimators_weights = None,
        fully_supervised = True,
        use_already_fitted = False,
        biadjecency_weights = 'uniform',
        stack_outputs = True,
    ):
        '''
        a heterogeneous ensemble of forests
        '''
        self.estimators = estimators
        self.estimators_weights = estimators_weights
        self.fully_supervised = fully_supervised
        self.use_already_fitted = use_already_fitted
        self.biadjecency_weights = biadjecency_weights
        self.stack_outputs = stack_outputs
        return

    def __getattr__(self, attr):
        return [getattr(i, attr) for i in self.estimators]

    def _iter_apply(self, method, *args, **kwargs):

        result = []
        for estim in self.estimators:
            result.append(getattr(estim, method)(*args, **kwargs))
        return result

    def apply(self, X, stack = None, **kwargs):

        result = self._iter_apply(method = 'apply', X = X)

        if stack is None:
            stack = self.stack_outputs
        #handle boosting case where returned array has 3 dims
        for arr in result:
            if len(arr.shape) >= 3:
                arr = arr.reshape(arr.shape[0], arr.shape[1]*arr.shape[2])

        if stack:
            result = hstack(result)
        return result

    def decision_path(self, X, stack = None, **kwargs):
        result = self._iter_apply(method = 'decision_path', X = X)

        if stack is None:
            stack = self.stack_outputs

        if stack:
            result0 = hstack([i[0] for i in result]).tocsr()
            result1 = hstack([i[1] for i in result])
        else:
            result0,result1 = result

        return result0, result1

    def node_biadjecency_matrix(self, X, use_weights = None, stack = None):

        '''
        use_weights can be "n_estimators", "weighted" or "uniform"
        unweighted returns biadjecency matrix filled with ones or zeros
        uniform makes the total sum of edge weights from all estimators the same
        wieghted
        '''

        if use_weights is None:
            use_weights = self.biadjecency_weights
        if stack is None:
            stack = self.stack_outputs

        #invert natural weights to make the sum of edges in each biadjacency uniform accross all estimators
        natural_weights = np.array([i.n_estimators for i in self.estimators])
        natural_weights = natural_weights/natural_weights.sum()

        uniform_weights = (1/natural_weights)
        uniform_weights = uniform_weights/uniform_weights.sum()

        if use_weights == "uniform":
            weights = uniform_weights
        elif use_weights == "weighted":
            weights = uniform_weights*self.estimators_weights_
        elif use_weights == "n_estimators":
            weights = np.ones((len(self.estimators_weights_),))
        else:
            raise ValueError(f'use_weights should be one of ["uniform", "weighted", "n_estimators", None], got {use_weights}')

        terminal_nodes = self.apply(X, stack=False)
        biadjs = []
        for i in range(len(terminal_nodes)):
            biadj_i = self.one_hot_node_embeddings_encoders_[i].transform(terminal_nodes[i])
            #scale and append
            biadjs.append(biadj_i*weights[i])

        if stack:
            biadjs = hstack(biadjs)

        return biadjs

# Cell

#export
class HeterogeneousMixedForest(BaseMixedForest):

    def fit(self, X, y = None, sample_weight = None):

        #get "natural" weights of estimators
        natural_weights = np.array([i.n_estimators for i in self.estimators])
        natural_weights = natural_weights/natural_weights.sum()
        #set estimator weights
        if self.estimators_weights is None:
            weights = natural_weights
        else:
            if len(self.estimators_weights) != len(self.estimators):
                raise ValueError(f'Shape mismatch between estimators and weights ({len(self.estimators)} != {len(self.estimators_weights)})')
            weights = np.array(weights)
            weights = weights/weights.sum()


        self.natural_weights_ = natural_weights
        self.estimators_weights_ = weights
        self.classes_ = classes
        self.multilabel_ = multilabel
        self.output_dim_ = output_dim

        #fit one hot encoders of the nodes
        terminal_nodes = super().apply(X, stack = False)
        self.one_hot_node_embeddings_encoders_ = [OneHotEncoder().fit(xi) for xi in terminal_nodes]
        return self

# Cell
class MixedForestRegressor(BaseMixedForest):

    def fit(self, X, y = None, sample_weight = None):

        #check if multidim output
        if len(y.shape) > 1:
            output_dim = y.shape[-1]
        else:
            output_dim = 1

        #get "natural" weights of estimators
        natural_weights = np.array([i.n_estimators for i in self.estimators])
        natural_weights = natural_weights/natural_weights.sum()
        #set estimator weights
        if self.estimators_weights is None:
            weights = natural_weights
        else:
            if len(self.estimators_weights) != len(self.estimators):
                raise ValueError(f'Shape mismatch between estimators and weights ({len(self.estimators)} != {len(self.estimators_weights)})')
            weights = np.array(weights)
            weights = weights/weights.sum()

        #fit estimators if needed
        for estim in self.estimators:
            if self.use_already_fitted:
                if not check_is_fitted(estim):
                    estim.fit(X, y = y, sample_weight = sample_weight)
                else:
                    warn(f"{estim} is already fitted and use_already_fitted was set to True in the constructor. The estimator won't be fitted, so ensure compatibility of inputs and outputs.")
            else:
                estim.fit(X, y = y, sample_weight = sample_weight)

        self.natural_weights_ = natural_weights
        self.estimators_weights_ = weights
        self.output_dim_ = output_dim

        #fit one hot encoders of the nodes
        terminal_nodes = super().apply(X, stack = False)
        self.one_hot_node_embeddings_encoders_ = [OneHotEncoder().fit(xi) for xi in terminal_nodes]
        return self


    def predict(self, X, aggregate = True):
        '''
        predicts classes
        '''

        weights = self.estimators_weights_
        result = super()._iter_apply(method = 'predict', X = X)
        sum_f = lambda a,b : weights[a]*result[a] + weights[b]*result[b]

        if aggregate:
            result = reduce(sum_f, range(len(result)))
        else:
            result = hstack([i.reshape(-1,1) for i in result])

        return result

# Cell
class MixedForestClassifier(BaseMixedForest):

    def fit(self, X, y = None, sample_weight = None):

        #check if multidim output
        if len(y.shape) > 1:
            output_dim = y.shape[-1]
        else:
            output_dim = 1

        if output_dim > 1:
            multilabel = True
        else:
            multilabel = False

        #get "natural" weights of estimators
        natural_weights = np.array([i.n_estimators for i in self.estimators])
        natural_weights = natural_weights/natural_weights.sum()
        #set estimator weights
        if self.estimators_weights is None:
            weights = natural_weights
        else:
            if len(self.estimators_weights) != len(self.estimators):
                raise ValueError(f'Shape mismatch between estimators and weights ({len(self.estimators)} != {len(self.estimators_weights)})')
            weights = np.array(weights)
            weights = weights/weights.sum()

        #fit estimators if needed
        for estim in self.estimators:
            if self.use_already_fitted:
                if not check_is_fitted(estim):
                    estim.fit(X, y = y, sample_weight = sample_weight)
                else:
                    warn(f"{estim} is already fitted and use_already_fitted was set to True in the constructor. The estimator won't be fitted, so ensure compatibility of inputs and outputs.")
            else:
                estim.fit(X, y = y, sample_weight = sample_weight)

        #check if classes are the same accross estimators
        if self.fully_supervised:
            classes = [estim.classes_ for estim in self.estimators]

            if multilabel:
                for i in range(len(classes)-1):
                    for j in range(len(classes[i])):
                        assert np.all(classes[i][j] == classes[i+1][j]), f'different classes in estimators: {classes[i]} and {classes[i+1]}'
                classes = classes[0]

            else:
                for i in range(len(classes)-1):
                    assert np.all(classes[i] == classes[i+1]), f'different classes in estimators: {classes[i]} and {classes[i+1]}'
                classes = classes[0]
        else:
            classes = None

        self.natural_weights_ = natural_weights
        self.estimators_weights_ = weights
        self.classes_ = classes
        self.multilabel_ = multilabel
        self.output_dim_ = output_dim

        #fit one hot encoders of the nodes
        terminal_nodes = super().apply(X, stack = False)
        self.one_hot_node_embeddings_encoders_ = [OneHotEncoder().fit(xi) for xi in terminal_nodes]
        return self

    def predict(self, X):
        '''
        predicts classes
        '''
        if self.multilabel_:
            probas = self.predict_proba(X)
            labels = []
            for i in range(len(probas)):
                indices = np.argmax(probas[i], axis = 1)
                labels.append(self.classes_[i][indices])
            labels = np.array(labels).T
        else:
            probas = self.predict_proba(X)
            indices = np.argmax(probas, axis = 1)
            labels = self.classes_[indices]

        return labels


    def predict_proba(self, X):
        '''
        predicts proba of each class
        '''
        weights = self.estimators_weights_
        result = self._iter_apply(method = 'predict_proba', X = X)
        sum_f = lambda a,b : weights[a]*result[a] + weights[b]*result[b]
        if self.multilabel_:
            #reverse the order of arrays in list
            res = [[] for _ in range(self.output_dim_)]
            for i in range(len(result)):
                for j in range(self.output_dim_):
                    res[j].append(result[i][j])

            result = []
            for dim_result in res:
                sum_f = lambda a,b : weights[a]*dim_result[a] + weights[b]*dim_result[b]
                res_i = reduce(sum_f, range(len(dim_result)))
                result.append(res_i)

        else:
            result = reduce(sum_f, range(len(result)))

        return result

# Cell
from sknetwork.embedding import GSVD, Spectral, SVD, PCA, RandomProjection, LouvainEmbedding, LouvainNE, ForceAtlas
from .utils import sparse_dot_product
class ForestBipartiteGraphTransformer(BaseEstimator):
    #fit models and graph embeddings on terminal node space
    #save louvain terminal node embeddings for inference
    #transform method will yield louvain embeddings of the point
    #create get_label method

    def __init__(
        self,
        forest_estimator,
        embedding_method = 'louvain',
        return_sparse = False,
        **embedding_kws
    ):
        self.forest_estimator = forest_estimator
        self.embedding_method = embedding_method
        self.return_sparse = return_sparse
        self.embedding_kws = embedding_kws
        return

    def __getattr__(self, attr):
        '''
        returns self.forest_estimator attribute if not found in class definition
        '''
        return getattr(self.forest_estimator, attr)

    def fit(self, X, y = None, **kwargs):

        #fit estimator
        self.forest_estimator.fit(X = X, y = y, **kwargs)
        # gets terminal nodes
        terminal_nodes = self.apply(X)
        #fit one hot encoders of the nodes
        self.one_hot_node_embeddings_encoders_ = OneHotEncoder().fit(terminal_nodes)
        #fits emedder
        self.graph_embedder_ = self._fit_embeddings(X, **kwargs)
        return self

    def apply(self, X, **kwargs):
        '''
        applies method "apply" ensuring shape compatibility
        '''
        X = self.forest_estimator.apply(X, **kwargs)
        #handle boosting case
        if len(X.shape) > 2:
            X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])
        return X

    def node_biadjecency_matrix(self, X):

        terminal_nodes = self.apply(X)
        #gets biadjecency matrix
        biadjecency_matrix = self.one_hot_node_embeddings_encoders_.transform(terminal_nodes)
        return biadjecency_matrix

    def _fit_embeddings(self, X, embeddings_method = None, **kwargs):

        '''
        fits embedder and returns fitted object
        '''

        if not kwargs:
            kwargs = self.embedding_kws

        if embeddings_method is None:
            embeddings_method = self.embedding_method

        if embeddings_method == 'louvain':
            method = LouvainEmbedding(**kwargs)
        else:
            raise ValueError(f'Suported methods are: ["louvain"], {embedding_method} was passed.')

        G = self.node_biadjecency_matrix(X)
        graph_embedder = method.fit(G)
        return graph_embedder


    def transform(self, X, return_sparse = None):
        '''
        makes a normalized dot product (linear combination) between terminal nodes of a specific datapoint and the terminal nodes embeddings
        '''
        if return_sparse is None:
            return_sparse = self.return_sparse

        X = self.node_biadjecency_matrix(X)
        terminal_node_embs = sparse.csr_matrix(self.graph_embedder_.embedding_col_)

        #TODO: implement flexible dot product function
        embs = sparse_dot_product(X, terminal_node_embs, terminal_node_embs.shape[-1])/self.n_estimators

        if return_sparse:
            embs = sparse.csr_matrix(embs)
        else:
            if sparse.issparse(embs):
                embs = embs.A
            else:
                pass

        return embs

# Cell

class EstimatorKernel(BaseEstimator, TransformerMixin):
    '''
    creates a kernel with some specified estimator.
    projection method will be performed according to projection_method.
    projection method can be a string refering to estimators method used to project,
    or a callable, that receives the estimator and X (vector to be projected) as the inputs.
    should return the projections of X according to estimator.
    norm will normalize vectors in matrices prior to applying dot products.
    '''
    def __init__(
        self,
        estimator:BaseEstimator,
        projection_method:Union[str,callable],
        nearest_neighbors_estimator,
        fit_neighbors_index:bool = True,
        n_neighbors:int = 30
    ):
        '''
        creates a kernel with some specified estimator.
        projection method will be performed according to projection_method.
        projection method can be a string refering to estimators method used to project,
        or a callable, that receives the estimator and X (vector to be projected) as the inputs.
        should return the projections of X according to estimator.
        norm will normalize vectors in matrices prior to applying dot products.
        '''
        self.estimator = estimator
        self.projection_method = projection_method
        self.nearest_neighbors_estimator = nearest_neighbors_estimator
        self.n_neighbors = n_neighbors
        self.fit_neighbors_index = fit_neighbors_index


    def __getattr__(self, attr):
        '''
        Allows accessing self.estimator attributes if not found in first object level
        '''
        return getattr(self.estimator, attr)

    def transform(self, X):
        '''
        projects X into new space, according to projection_method
        '''

        if callable(self.projection_method):
            return self.projection_method(self.estimator, X)
        else:
            return getattr(self.estimator, self.projection_method)(X)

    def fit(self, X, y = None, save_values = None, **kwargs):
        '''
        X is the feature space,
        y is used only for supervised Kernels
        save_values are values associated with each "Embeding". During transform,
        the values of saved_values are retrieved according to indexes returned by Nearest Neighbor query
        '''
        if not save_values is None:
            if not len(save_values) == len(X):
                raise IndexError(f'X and save_values must have the same shape along the first dimension. Got {X.shape} and {save_values.shape}')

        self.estimator.fit(X, y, **kwargs)

        if self.fit_neighbors_index:
            #make space transformation
            train_projection_space_ = self.transform(X) #saves projection space of X in train

            if save_values is None:
                save_values = np.empty((X.shape[0], 0)) #empty array

            #fit index
            self.nearest_neighbors_estimator.fit(train_projection_space_)

            #save states
            #self.train_projection_space_ = train_projection_space_ #is it really necessary to save this? possibly yieds memory overhead
            self.train_projection_values_ = save_values

        return self


    def kneighbors(self, X = None, n_neighbors = None, return_distance = True):
        '''
        runs nearest neighbor search in the transformed space index
        '''

        if not self.fit_neighbors_index:
            raise AttributeError('This method is only available when fit_neighbors_index is set to True in the constructor')

        if n_neighbors is None:
            n_neighbors = self.n_neighbors

        #make space transformation
        X = self.transform(X)
        #query in index
        result = self.nearest_neighbors_estimator.kneighbors(X, n_neighbors = n_neighbors, return_distance = return_distance)

        return result #dist, idxs

    def kneighbors_graph(self, X = None, n_neighbors = None, mode = 'similarity'):

        if not self.fit_neighbors_index:
            raise AttributeError('This method is only available when fit_neighbors_index is set to True in the constructor')

        if n_neighbors is None:
            n_neighbors = self.n_neighbors

        #make space transformation
        if not X is None:
            X = self.transform(X)
        #query in index
        graph = self.nearest_neighbors_estimator.kneighbors_graph(X, n_neighbors = n_neighbors, mode = mode)
        return graph

    def query(self, X = None, n_neighbors = None):
        '''
        same as kneighbors, but instead of returning indexes, returns values in self.train_projection_values_
        '''
        if not self.fit_neighbors_index:
            raise AttributeError('This method is only available when fit_neighbors_index is set to True in the constructor')

        dist, idxs = self.kneighbors(X, n_neighbors, return_distance = True)

        if hasattr(self.train_projection_values_, 'iloc'):
            values = [self.train_projection_values_.iloc[idx] for idx in idxs]

        else:
            values = [self.train_projection_values_[idx] for idx in idxs]

        return values, dist

    def update_space(self, X, save_values = None):
        '''
        updates self.train_projection_space_ and self.train_projection_values_ with new data.
        new values are found running self.transform on X
        '''
        if not self.fit_neighbors_index:
            raise AttributeError('This method is only available when fit_neighbors_index is set to True in the constructor')

        X = self.transform(X)

        self.train_projection_space_ = vstack([self.train_projection_space_, X])

        if save_values is None:
            save_values = np.empty((X.shape[0], self.train_projection_values_.shape[-1])) #empty array


        #refit knn index
        self.nearest_neighbors_estimator.fit(self.train_projection_space_)

        self.train_projection_values_ = vstack([self.train_projection_values_, save_values])
        return self

# Cell
class JaccardForestKernel(EstimatorKernel):
    '''
    A Space tranformation performed based on Forest transformations.
    Can be supervised or not (CARTs, RandomTreeEmbeddings, Boosted trees...)

    the embedding_space is sparse and can be defined as the `decision_path` space or `terminal_nodes`
    space.
    '''

    def __init__(
        self,
        estimator:BaseEstimator,
        n_neighbors:int = 30,
        fit_neighbors_index:bool = True,
        index_time_params:dict={'M': 30, 'indexThreadQty': 4, 'efConstruction': 100, 'post': 0},
        query_time_params:dict={'efSearch': 100},
        verbose:bool = False
    ):

        #save init params for sklearn consistency
        self.n_neighbors = n_neighbors
        self.index_time_params=index_time_params
        self.query_time_params=query_time_params
        self.verbose=verbose
        self.fit_neighbors_index=fit_neighbors_index

        #instantiate jacard distance nearest neighbor index
        nn_obj = FastJaccardNN(
            n_neighbors = n_neighbors,
            index_time_params=index_time_params,
            query_time_params=query_time_params,
            verbose=verbose,
        )

        super().__init__(
            estimator = estimator,
            projection_method=None,
            nearest_neighbors_estimator=nn_obj,
            fit_neighbors_index=fit_neighbors_index,
            n_neighbors=n_neighbors
        )

        return

    def transform(self, X):

        X = self.estimator.apply(X)
        #handle boosting case
        if len(X.shape) > 2:
            X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])

        if hasattr(self, 'one_hot_node_embeddings_encoder_'):
            X = self.one_hot_node_embeddings_encoder_.transform(X)

        else:
            self.one_hot_node_embeddings_encoder_ = OneHotEncoder().fit(X)
            X = self.one_hot_node_embeddings_encoder_.transform(X)

        return X

# Cell
class CategoricalLinearKernel(EstimatorKernel):
    '''
    Linear model kernel recommended for high cardinality one hot encoded categorical variables.
    kernel space is defined by liner model coefficients indexed by the nonzero elements
    of X

    If encode is set to true, a customized onehotencoder will encode the categorical input.

    This kernel will only work if the output of the one hot encoded vectors  have always the same number
    of nonzero elements (equal to the number of categorical features). Thus, its recomended to use the default
    encoder, because it asserts this condition is met during one hot encoding
    '''

    def __init__(
        self,
        estimator,
        n_neighbors=30,
        fit_neighbors_index = True,
        encode = False,
        n_components = None,
        index_time_params={'M': 30, 'indexThreadQty': 8, 'efConstruction': 100, 'post': 0},
        query_time_params={'efSearch': 100},
        verbose=False,
        **pcakwargs
    ):

        self.encode = encode
        self.n_components = n_components
        self.pcakwargs = pcakwargs
        self.estimator = estimator

        #save init params for sklearn consistency
        self.n_neighbors = n_neighbors
        self.index_time_params=index_time_params
        self.query_time_params=query_time_params
        self.verbose=verbose
        self.fit_neighbors_index=fit_neighbors_index

        #instantiate jacard distance nearest neighbor index
        nn_obj = FastL2NN(
            n_neighbors = n_neighbors,
            index_time_params=index_time_params,
            query_time_params=query_time_params,
            verbose=verbose,
        )

        super().__init__(
            estimator = estimator,
            projection_method=None,
            nearest_neighbors_estimator=nn_obj,
            fit_neighbors_index=fit_neighbors_index,
            n_neighbors=n_neighbors
        )

        return

    def fit(self, X, y = None, save_values = None, **kwargs):

        if self.encode:
            if not self.n_components is None:
                self.estimator = make_pipeline(RobustEncoder(), self.estiamtor, PCA(self.n_components, **self.pcakwargs))
            else:
                self.estimator = make_pipeline(RobustEncoder(), self.estimator)
        else:
            if not self.n_components is None:
                self.estimator = make_pipeline(self.estiamtor, PCA(self.n_components, **self.pcakwargs))
            else:
                pass

        return super().fit(X, y, save_values, **kwargs)

    def transform(self, X):
        '''
        multiplies sparse vector to its coef_ s from linear model.
        if multiclass classification, the number of final features will be
        n*original_n_features_before_one_hot_encoding
        '''

        if self.encode:
            coefs = self.estimator[-1].coef_
            X = self.estimator[0].transform(X)
        else:
            coefs = self.estimator.coef_

        #create attr if it does now exist yet:
        #this line is supposed to run only during fit call
        if not hasattr(self,'dim_embeddings_'):
            self.dim_embeddings_ = len(X[0].data)

        if len(coefs.shape) == 1:
            coefs = coefs.reshape(1,-1)

        embeddings = []
        for dim in range(coefs.shape[0]):
            #assumes all rows have the same ammount of nonzero elements
            dim_embeddings = coefs[dim, X.nonzero()[1]].reshape(X.shape[0], self.dim_embeddings_)
            embeddings.append(dim_embeddings)

        return hstack(embeddings)


# Cell
class ClassificationLinearBottleneck(MLPClassifier):
    '''
    Linear boottleneck of a classification task, usefull for dimensionality reduction
    or densification of sparse representations.
    '''
    def __init__(
        self,
        n_components = 2,
        solver='adam',
        alpha=0.0001,
        batch_size='auto',
        learning_rate='constant',
        learning_rate_init=0.001,
        power_t=0.5,
        max_iter=200,
        shuffle=True,
        random_state=None,
        tol=0.0001,
        verbose=False,
        warm_start=False,
        momentum=0.9,
        nesterovs_momentum=True,
        early_stopping=False,
        validation_fraction=0.1,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-08,
        n_iter_no_change=10,
        max_fun=15000,
    ):

        #set attributes, some will be overriden in super().__init__ call
        self.solver = solver
        self.alpha = alpha
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.learning_rate_init = learning_rate_init
        self.power_t = power_t
        self.max_iter = max_iter
        self.shuffle = shuffle
        self.random_state = random_state
        self.tol = tol
        self.verbose = verbose
        self.warm_start = warm_start
        self.momentum = momentum
        self.nesterovs_momentum = nesterovs_momentum
        self.early_stopping = early_stopping
        self.validation_fraction = validation_fraction
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        self.n_iter_no_change = n_iter_no_change
        self.max_fun = max_fun

        super().__init__(hidden_layer_sizes = (n_components,), activation = 'identity', **self.__dict__)
        self.n_components = n_components
        return

    def transform(self, X, **kwargs):
        '''
        projects inputs to have size (n_samples, n_components)
        '''
        return _get_sklearn_mlp_activations(self, X)


class RegressionLinearBottleneck(MLPRegressor):
    '''
    Linear boottleneck of a classification task, usefull for dimensionality reduction
    or densification of sparse representations.
    '''
    def __init__(
        self,
        n_components = 2,
        solver='adam',
        alpha=0.0001,
        batch_size='auto',
        learning_rate='constant',
        learning_rate_init=0.001,
        power_t=0.5,
        max_iter=200,
        shuffle=True,
        random_state=None,
        tol=0.0001,
        verbose=False,
        warm_start=False,
        momentum=0.9,
        nesterovs_momentum=True,
        early_stopping=False,
        validation_fraction=0.1,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-08,
        n_iter_no_change=10,
        max_fun=15000,
    ):

        #set attributes, some will be overriden in super().__init__ call
        self.solver = solver
        self.alpha = alpha
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.learning_rate_init = learning_rate_init
        self.power_t = power_t
        self.max_iter = max_iter
        self.shuffle = shuffle
        self.random_state = random_state
        self.tol = tol
        self.verbose = verbose
        self.warm_start = warm_start
        self.momentum = momentum
        self.nesterovs_momentum = nesterovs_momentum
        self.early_stopping = early_stopping
        self.validation_fraction = validation_fraction
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        self.n_iter_no_change = n_iter_no_change
        self.max_fun = max_fun

        super().__init__(hidden_layer_sizes = (n_components,), activation = 'identity', **self.__dict__)
        self.n_components = n_components
        return

    def transform(self, X, **kwargs):
        '''
        projects inputs to have size (n_samples, n_components)
        '''
        return _get_sklearn_mlp_activations(self, X)

# Cell
def _get_sklearn_mlp_activations(self, X, output_layer = -2):
    hidden_layer_sizes = self.hidden_layer_sizes
    if not hasattr(hidden_layer_sizes, "__iter__"):
        hidden_layer_sizes = [hidden_layer_sizes]
    hidden_layer_sizes = list(hidden_layer_sizes)
    layer_units = [X.shape[1]] + hidden_layer_sizes + \
        [self.n_outputs_]
    activations = [X]
    for i in range(self.n_layers_ - 1):
        activations.append(np.empty((X.shape[0],
                                     layer_units[i + 1])))
    self._forward_pass(activations)
    return activations[output_layer]

# Cell
class MLPKernel(EstimatorKernel):

    '''
    returns the output of last hidden layer (before softmax/linear layer)
    as space projection.

    Recomended for dimensionality reduction and Context specific bag of words task
    '''
    def __init__(self, estimator, output_layer = -2, norm='l2'):

        self.projection_method = partial(_get_sklearn_mlp_activations, output_layer = output_layer)
        self.estimator = estimator
        self.output_layer = output_layer
        self.norm = norm
        return



# Cell
class BOWKernel(MLPKernel):
    '''
    `MLPKernel` Alias, intended for Bag Of Words application.
    Generates supervised embeddings (context specific embeddings)
    '''
    pass

# Cell

class DiscretizedTargetKernel(EstimatorKernel):

    def __init__(
        self,
        estimator,
        n_bins=10,
        encode='ordinal',
        strategy='kmeans',
        n_neighbors=30,
        fit_neighbors_index = True,
        index_time_params={'indexThreadQty': 8, 'efConstruction': 100},
        query_time_params={'efSearch': 100},
        verbose=False,
    ):


        #save init params for sklearn consistency
        self.n_neighbors = n_neighbors
        self.index_time_params=index_time_params
        self.query_time_params=query_time_params
        self.verbose=verbose
        self.fit_neighbors_index=fit_neighbors_index
        self.n_bins=n_bins
        self.encode=encode
        self.strategy=strategy

        #instantiate KL Divergence nearest neighbor index
        nn_obj = FastKLDivNN(
            n_neighbors = n_neighbors,
            index_time_params=index_time_params,
            query_time_params=query_time_params,
            verbose=verbose,
        )

        super().__init__(
            estimator = estimator,
            projection_method='predict_proba',
            nearest_neighbors_estimator=nn_obj,
            fit_neighbors_index=fit_neighbors_index,
            n_neighbors=n_neighbors
        )

        return

    def fit(self, X, y = None, save_values = None, **kwargs):

        discretizer = KBinsDiscretizer(
            n_bins = self.n_bins,
            encode = self.encode,
            strategy = self.strategy,

        )

        y = discretizer.fit_transform(y)

        self.discretizer = discretizer
        return super().fit(X, y, save_values, **kwargs)

    def transform(self, X):

        X = self.estimator.predict_proba(X)
        #handle multiclass (multi dimensional joint dist)
        if isinstance(X, list):
            X = hstack(X)
        return X

